{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML by simple linear regression and only 1 column (living area(total_area_sqm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'price', 'property_type', 'subproperty_type', 'region',\n",
       "       'province', 'locality', 'zip_code', 'latitude', 'longitude',\n",
       "       'construction_year', 'total_area_sqm', 'surface_land_sqm',\n",
       "       'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen', 'fl_furnished',\n",
       "       'fl_open_fire', 'fl_terrace', 'terrace_sqm', 'fl_garden', 'garden_sqm',\n",
       "       'fl_swimming_pool', 'fl_floodzone', 'state_building',\n",
       "       'primary_energy_consumption_sqm', 'epc', 'heating_type',\n",
       "       'fl_double_glazing', 'cadastral_income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"C:/Users/becod/AI/my-projects/immo-eliza-ml-Majid/immo-eliza-ml-Majid/becode_properties.csv\"\n",
    "\n",
    "basic_preprocessed_dataset = \"C:/Users/becod/AI/my-projects/immo-eliza-ml-Majid/immo-eliza-ml-Majid/basic_preprocessed_dataset.csv\"\n",
    "\n",
    "# Define the path for saving the preprocessed dataset\n",
    "advance_preprocessed_dataset = \"C:/Users/becod/AI/my-projects/immo-eliza-ml-Majid/immo-eliza-ml-Majid/advance_preprocessed_dataset.csv\"\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (75511, 30)\n",
      "After removing duplicates: (75511, 30)\n",
      "After dropping unnecessary columns: (75511, 22)\n",
      "After dropping rows with NaN in ['price', 'total_area_sqm', 'nbr_bedrooms']: (66349, 22)\n",
      "After filtering rows with zero in ['total_area_sqm', 'nbr_bedrooms']: (66349, 22)\n",
      "NaNs replaced with 0 for columns: ['fl_furnished', 'fl_open_fire', 'fl_terrace', 'fl_garden', 'fl_swimming_pool', 'fl_floodzone', 'fl_double_glazing']\n",
      "NaNs filled with random distribution for columns: ['construction_year', 'total_area_sqm', 'surface_land_sqm', 'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen', 'terrace_sqm', 'garden_sqm', 'state_building', 'epc', 'latitude', 'longitude']\n",
      "After removing outliers in 'total_area_sqm': (62553, 22)\n"
     ]
    }
   ],
   "source": [
    "class BasicPreprocessing:\n",
    "    def __init__(self, df):\n",
    "        # Initialize the class with a copy of the DataFrame\n",
    "        self.dataset = df.copy()\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to remove duplicate rows from the dataset\n",
    "    def drop_duplicates(self):\n",
    "        print(\"Original dataset size:\", self.dataset.shape)\n",
    "        self.dataset = self.dataset.drop_duplicates()\n",
    "        print(\"After removing duplicates:\", self.dataset.shape)\n",
    "        \n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to drop unnecessary columns from the dataset\n",
    "    def drop_columns(self):\n",
    "        columns_to_drop = ['id', 'subproperty_type', 'region', 'province', 'locality', \n",
    "                           'primary_energy_consumption_sqm', 'heating_type', 'cadastral_income']\n",
    "        self.dataset = self.dataset.drop(columns=columns_to_drop)\n",
    "        print(\"After dropping unnecessary columns:\", self.dataset.shape)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to drop rows with NaN values in specific columns\n",
    "    def drop_na(self):\n",
    "        columns_to_check_na = ['price', 'total_area_sqm', 'nbr_bedrooms']\n",
    "        self.dataset = self.dataset.dropna(subset=columns_to_check_na)\n",
    "        print(f\"After dropping rows with NaN in {columns_to_check_na}: {self.dataset.shape}\")\n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to filter out rows where specific columns have zero values\n",
    "    def filter_non_zero(self):\n",
    "        columns_to_check_zero = ['total_area_sqm', 'nbr_bedrooms']\n",
    "        condition = (self.dataset[columns_to_check_zero] > 0).all(axis=1)\n",
    "        self.dataset = self.dataset[condition]\n",
    "        print(f\"After filtering rows with zero in {columns_to_check_zero}: {self.dataset.shape}\")\n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to replace specific missing values with NaN\n",
    "    def replace_special_missing(self):\n",
    "        self.dataset.replace([\"\", \" \", \"MISSING\", 0], np.nan, inplace=True)\n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to fill NaN values with zero in specified columns\n",
    "    def fill_na_with_zero(self):\n",
    "        columns_to_fill_zero = [\n",
    "            'fl_furnished', 'fl_open_fire', 'fl_terrace', 'fl_garden',\n",
    "            'fl_swimming_pool', 'fl_floodzone', 'fl_double_glazing'\n",
    "        ]\n",
    "        self.dataset[columns_to_fill_zero] = self.dataset[columns_to_fill_zero].fillna(0)\n",
    "        print(f\"NaNs replaced with 0 for columns: {columns_to_fill_zero}\")\n",
    "    #------------------------------------------------------------------------\n",
    "    # Method to fill NaN values with a weighted random distribution in specified columns\n",
    "    def fill_na_with_random_distribution(self):\n",
    "        columns_to_fill_random_distribution = [\n",
    "            'construction_year', 'total_area_sqm', 'surface_land_sqm',\n",
    "            'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen',\n",
    "            'terrace_sqm', 'garden_sqm', 'state_building', 'epc','latitude', 'longitude'\n",
    "        ]\n",
    "        for column in columns_to_fill_random_distribution:\n",
    "            value_counts = self.dataset[column].value_counts(normalize=True)\n",
    "            values = value_counts.index.tolist()\n",
    "            probabilities = value_counts.values\n",
    "            self.dataset[column] = self.dataset[column].apply(\n",
    "                lambda x: np.random.choice(values, p=probabilities) if pd.isna(x) else x\n",
    "            )\n",
    "        print(f\"NaNs filled with random distribution for columns: {columns_to_fill_random_distribution}\")\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    def remove_outliers(self):\n",
    "        Q1 = self.dataset['total_area_sqm'].quantile(0.25)\n",
    "        Q3 = self.dataset['total_area_sqm'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        self.dataset = self.dataset[(self.dataset['total_area_sqm'] >= lower_bound) & (self.dataset['total_area_sqm'] <= upper_bound)]\n",
    "        print(\"After removing outliers in 'total_area_sqm':\", self.dataset.shape)\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Main method to process the dataset through all the steps\n",
    "    def process_basic(self):\n",
    "        \n",
    "        self.drop_duplicates()  # Remove duplicate rows\n",
    "        self.drop_columns()  # Drop unnecessary columns\n",
    "        self.replace_special_missing()  # Replace special missing values with NaN\n",
    "        self.drop_na()  # Drop rows with NaN in essential columns\n",
    "        self.filter_non_zero()  # Filter out rows with zero values in key columns\n",
    "        self.fill_na_with_zero()  # Fill NaN values with zero in specific columns\n",
    "        self.fill_na_with_random_distribution()  # Fill NaN values using a random distribution\n",
    "        self.remove_outliers()  # Remove outliers\n",
    "\n",
    "        return self.dataset  # Return the processed DataFrame\n",
    "\n",
    "    \n",
    "# Instantiate the class and process the DataFrame\n",
    "basic_preprocessor = BasicPreprocessing(df)\n",
    "basic_preprocessed_data = basic_preprocessor.process_basic()\n",
    "\n",
    "# Save the cleaned data to a CSV file\n",
    "basic_preprocessed_data.to_csv(basic_preprocessed_dataset, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>total_area_sqm</th>\n",
       "      <th>surface_land_sqm</th>\n",
       "      <th>nbr_frontages</th>\n",
       "      <th>nbr_bedrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_open_fire</th>\n",
       "      <th>fl_terrace</th>\n",
       "      <th>terrace_sqm</th>\n",
       "      <th>fl_garden</th>\n",
       "      <th>garden_sqm</th>\n",
       "      <th>fl_swimming_pool</th>\n",
       "      <th>fl_floodzone</th>\n",
       "      <th>state_building</th>\n",
       "      <th>epc</th>\n",
       "      <th>fl_double_glazing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225000.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>2050</td>\n",
       "      <td>51.217172</td>\n",
       "      <td>4.379982</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TO_RENOVATE</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>335000.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>1070</td>\n",
       "      <td>50.842043</td>\n",
       "      <td>4.334543</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AS_NEW</td>\n",
       "      <td>G</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>501000.0</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>2275</td>\n",
       "      <td>51.238312</td>\n",
       "      <td>4.817192</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>982700.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>1410</td>\n",
       "      <td>50.754677</td>\n",
       "      <td>3.092787</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AS_NEW</td>\n",
       "      <td>A+</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>548514.0</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>1700</td>\n",
       "      <td>50.456027</td>\n",
       "      <td>4.686474</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AS_NEW</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price property_type  zip_code   latitude  longitude  construction_year  \\\n",
       "0  225000.0     APARTMENT      2050  51.217172   4.379982             1963.0   \n",
       "2  335000.0     APARTMENT      1070  50.842043   4.334543             1900.0   \n",
       "3  501000.0         HOUSE      2275  51.238312   4.817192             2024.0   \n",
       "4  982700.0     APARTMENT      1410  50.754677   3.092787             2022.0   \n",
       "5  548514.0         HOUSE      1700  50.456027   4.686474             1991.0   \n",
       "\n",
       "   total_area_sqm  surface_land_sqm  nbr_frontages  nbr_bedrooms  ...  \\\n",
       "0           100.0             294.0            2.0           2.0  ...   \n",
       "2           142.0             200.0            2.0           3.0  ...   \n",
       "3           187.0             505.0            2.0           3.0  ...   \n",
       "4           169.0             305.0            2.0           2.0  ...   \n",
       "5           187.0             710.0            4.0           3.0  ...   \n",
       "\n",
       "  fl_open_fire  fl_terrace  terrace_sqm  fl_garden  garden_sqm  \\\n",
       "0          0.0         1.0          5.0        0.0        79.0   \n",
       "2          0.0         1.0         28.0        0.0       352.0   \n",
       "3          0.0         0.0          6.0        0.0        45.0   \n",
       "4          0.0         1.0         20.0        1.0       142.0   \n",
       "5          0.0         0.0         13.0        0.0        72.0   \n",
       "\n",
       "   fl_swimming_pool  fl_floodzone  state_building  epc fl_double_glazing  \n",
       "0               0.0           0.0     TO_RENOVATE    C               1.0  \n",
       "2               0.0           1.0          AS_NEW    G               0.0  \n",
       "3               0.0           1.0            GOOD    A               0.0  \n",
       "4               0.0           0.0          AS_NEW   A+               0.0  \n",
       "5               0.0           1.0          AS_NEW    A               0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'property_type', 'zip_code', 'latitude', 'longitude',\n",
       "       'construction_year', 'total_area_sqm', 'surface_land_sqm',\n",
       "       'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen', 'fl_furnished',\n",
       "       'fl_open_fire', 'fl_terrace', 'terrace_sqm', 'fl_garden', 'garden_sqm',\n",
       "       'fl_swimming_pool', 'fl_floodzone', 'state_building', 'epc',\n",
       "       'fl_double_glazing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_preprocessed_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advance Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after encoding and standardization.\n"
     ]
    }
   ],
   "source": [
    "class AdvancedPreprocessing:\n",
    "    #------------------------------------------------------------------------\n",
    "    def __init__(self, basic_preprocessed_data):\n",
    "        self.dataset = basic_preprocessed_data.copy()\n",
    "        self.existing_columns_to_standardize = ['total_area_sqm', 'surface_land_sqm', 'terrace_sqm', 'garden_sqm']\n",
    "        self.new_columns_to_standardize = ['bedrooms_per_sqm']\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    def create_new_features(self):\n",
    "\n",
    "        #--------------------- bedrooms_per_sqm\n",
    "        # crete new feature 'bedrooms_per_sqm' to compute density of rooms per square meter of area\n",
    "        # Replace zero in 'total_area_sqm' with a minimum value (e.g., 1) to avoid division by zero\n",
    "        self.dataset.loc[:, 'total_area_sqm'] = self.dataset['total_area_sqm'].replace(0, 1)\n",
    "\n",
    "        # Create new feature 'bedrooms_per_sqm'\n",
    "        self.dataset['bedrooms_per_sqm'] = self.dataset['nbr_bedrooms'] / self.dataset['total_area_sqm']\n",
    "        self.dataset.loc[:, 'bedrooms_per_sqm'] = self.dataset['bedrooms_per_sqm'].replace([np.inf, -np.inf], np.nan)  # Replace inf and -inf with NaN\n",
    "        mean_value_bedrooms = self.dataset['bedrooms_per_sqm'].mean()\n",
    "        self.dataset.loc[:, 'bedrooms_per_sqm'] = self.dataset['bedrooms_per_sqm'].fillna(mean_value_bedrooms)  # Fill NaNs with mean\n",
    "\n",
    "        # Standardize the values\n",
    "        scaler_bedrooms = StandardScaler()\n",
    "        self.dataset['bedrooms_per_sqm_scaled'] = scaler_bedrooms.fit_transform(self.dataset[['bedrooms_per_sqm']])\n",
    "\n",
    "        #--------------------- total_area_per_bedroom\n",
    "        # create new feature 'total_area_per_bedroom' to compute average area per bedroom in the property\n",
    "        # Replace zero in 'nbr_bedrooms' with NaN to avoid division by zero\n",
    "        self.dataset.loc[:, 'nbr_bedrooms'] = self.dataset['nbr_bedrooms'].replace(0, np.nan)\n",
    "\n",
    "        # Create new feature 'total_area_per_bedroom'\n",
    "        self.dataset['total_area_per_bedroom'] = self.dataset['total_area_sqm'] / self.dataset['nbr_bedrooms']\n",
    "        self.dataset.loc[:, 'total_area_per_bedroom'] = self.dataset['total_area_per_bedroom'].replace([np.inf, -np.inf], np.nan)  # Replace inf and -inf with NaN\n",
    "        mean_value_area_per_bedroom = self.dataset['total_area_per_bedroom'].mean()\n",
    "        self.dataset.loc[:, 'total_area_per_bedroom'] = self.dataset['total_area_per_bedroom'].fillna(mean_value_area_per_bedroom)  # Fill NaNs with mean\n",
    "\n",
    "        # Standardize the values\n",
    "        scaler_bedroom_area = StandardScaler()\n",
    "        self.dataset['total_area_per_bedroom_scaled'] = scaler_bedroom_area.fit_transform(self.dataset[['total_area_per_bedroom']])\n",
    "\n",
    "        #--------------------- price_per_total_area\n",
    "        # crete new feature 'price_per_total_area'\n",
    "        self.dataset['price_per_total_area'] = self.dataset['price'] / self.dataset['total_area_sqm']  # create new feature\n",
    "        self.dataset.loc[:, 'price_per_total_area'] = self.dataset['price_per_total_area'].replace([np.inf, -np.inf], np.nan)  # Replace inf and -inf with NaN\n",
    "        mean_price_per_area = self.dataset['price_per_total_area'].mean()\n",
    "        self.dataset.loc[:, 'price_per_total_area'] = self.dataset['price_per_total_area'].fillna(mean_price_per_area)  # Fill NaNs with mean\n",
    "\n",
    "        # Standardize the values\n",
    "        scaler_price = StandardScaler()\n",
    "        self.dataset['price_per_total_area_scaled'] = scaler_price.fit_transform(self.dataset[['price_per_total_area']])\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    def encode_standardize_categories_features(self):\n",
    "\n",
    "        #--------------------- encode and standardize 'equipped_kitchen'\n",
    "        kitchen_order = ['unknown', 'NOT_INSTALLED', \"USA_UNINSTALLED\", \"SEMI_EQUIPPED\", \"USA_SEMI_EQUIPPED\", \n",
    "                        \"INSTALLED\", \"USA_INSTALLED\", \"HYPER_EQUIPPED\", \"USA_HYPER_EQUIPPED\"]\n",
    "        \n",
    "        # Check if all unique values in the column are in the defined order\n",
    "        kitchen_values = set(self.dataset['equipped_kitchen'].unique())\n",
    "        unmatched_values = kitchen_values - set(kitchen_order)\n",
    "        if unmatched_values:\n",
    "            # print(f\"Warning: The following values in 'equipped_kitchen' are not in the defined order list: {unmatched_values}\")\n",
    "            # Replace unmatched values with 'unknown'\n",
    "            self.dataset['equipped_kitchen'].replace(list(unmatched_values), 'unknown', inplace=True)\n",
    "\n",
    "        # Proceed with encoding\n",
    "        encoder_kit = OrdinalEncoder(categories=[kitchen_order])\n",
    "        self.dataset['kitchen_type_encoded'] = encoder_kit.fit_transform(self.dataset[['equipped_kitchen']])\n",
    "        \n",
    "        # Standardize 'kitchen_type_encoded' and convert it to integer\n",
    "        scaler_kitchen = StandardScaler()\n",
    "        self.dataset['kitchen_type_encoded'] = scaler_kitchen.fit_transform(self.dataset[['kitchen_type_encoded']])\n",
    "        self.dataset['kitchen_type_encoded'] = self.dataset['kitchen_type_encoded'].astype(int)  # Convert to integer\n",
    "\n",
    "        #--------------------- encode and standardize 'state_building'\n",
    "        building_order = ['unknown', \"TO_RESTORE\", \"TO_RENOVATE\", \"TO_BE_DONE_UP\", \"GOOD\", \"JUST_RENOVATED\", \"AS_NEW\"]\n",
    "\n",
    "        # Check if all unique values in the column are in the defined order\n",
    "        building_values = set(self.dataset['state_building'].unique())\n",
    "        unmatched_values = building_values - set(building_order)\n",
    "        if unmatched_values:\n",
    "            # print(f\"Warning: The following values in 'state_building' are not in the defined order list: {unmatched_values}\")\n",
    "            # Replace unmatched values with 'unknown'\n",
    "            self.dataset['state_building'].replace(list(unmatched_values), 'unknown', inplace=True)\n",
    "\n",
    "        # Proceed with encoding\n",
    "        encoder_bul = OrdinalEncoder(categories=[building_order])\n",
    "        self.dataset['Bulding_sta_encoded'] = encoder_bul.fit_transform(self.dataset[['state_building']])\n",
    "        \n",
    "        # Standardize 'Bulding_sta_encoded'and convert it to integer\n",
    "        scaler_building = StandardScaler()\n",
    "        self.dataset['Bulding_sta_encoded'] = scaler_building.fit_transform(self.dataset[['Bulding_sta_encoded']])\n",
    "        self.dataset['Bulding_sta_encoded'] = self.dataset['Bulding_sta_encoded'].astype(int)  # Convert to integer\n",
    "                \n",
    "        #--------------------- encode and standardize 'epc'\n",
    "        epc_order = ['G', 'F', 'E', 'D', 'C', 'B', 'A', 'A+', 'A++']\n",
    "        \n",
    "        # Check if all unique values in the column are in the defined order\n",
    "        epc_values = set(self.dataset['epc'].unique())\n",
    "        unmatched_values = epc_values - set(epc_order)\n",
    "        if unmatched_values:\n",
    "            # print(f\"Warning: The following values in 'epc' are not in the defined order list: {unmatched_values}\")\n",
    "            # Replace unmatched values with 'F' as a default (lowest category)\n",
    "            self.dataset['epc'].replace(list(unmatched_values), 'F', inplace=True)\n",
    "\n",
    "        # Proceed with encoding\n",
    "        encoder_epc = OrdinalEncoder(categories=[epc_order])\n",
    "        self.dataset['epc_encoded'] = encoder_epc.fit_transform(self.dataset[['epc']])\n",
    "        \n",
    "        # Standardize 'epc_encoded' and convert it to integer\n",
    "        scaler_epc = StandardScaler()\n",
    "        self.dataset['epc_encoded'] = scaler_epc.fit_transform(self.dataset[['epc_encoded']])\n",
    "        self.dataset['epc_encoded'] = self.dataset['epc_encoded'].astype(int)  # Convert to integer\n",
    "\n",
    "        print(\"Data after encoding and standardization.\")\n",
    "        \n",
    "    #------------------------------------------------------------------------\n",
    "  \n",
    "    def categorize_construction_year(self):\n",
    "         \n",
    "        # Replace missing values with the median of the column\n",
    "        self.dataset['construction_year'] = self.dataset['construction_year'].fillna(self.dataset['construction_year'].median())\n",
    "\n",
    "        # Create a new column 'construction_category' with zip_code // 10\n",
    "        self.dataset['construction_category'] = (self.dataset['construction_year'] // 10).astype(int)\n",
    "        # for bulding older that 1900, same category in 1900\n",
    "        self.dataset['construction_category'] = self.dataset['construction_category'].apply(lambda x: max(x, 190))\n",
    "\n",
    "        # Initialize the OneHotEncoder with the updated parameter\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        # Fit and transform the zip_code column\n",
    "        construction_category_encoded = encoder.fit_transform(self.dataset[['construction_category']])\n",
    "        \n",
    "        # Create a DataFrame from the encoded zip codes with appropriate column names\n",
    "        encoded_df = pd.DataFrame(construction_category_encoded, columns=encoder.get_feature_names_out(['construction_category']))\n",
    "        \n",
    "        # Combine the original dataset with the new encoded columns\n",
    "        self.dataset = pd.concat([self.dataset.reset_index(drop=True), encoded_df], axis=1)\n",
    "        \n",
    "        # Optionally drop the original 'zip_code' column if not needed\n",
    "        # self.dataset.drop(['zip_code'], axis=1, inplace=True)\n",
    "\n",
    "        # def year_category(year):\n",
    "        #     if year < 1950:\n",
    "        #         return 'veryold' \n",
    "        #     elif 1950 <= year < 1970:\n",
    "        #         return 'old'  \n",
    "        #     elif 1970 <= year < 1990:\n",
    "        #         return 'midold' \n",
    "        #     elif 1990 <= year < 2010:\n",
    "        #         return 'mid' \n",
    "        #     else:\n",
    "        #         return 'new' \n",
    "\n",
    "        # self.dataset['construction_category'] = self.dataset['construction_year'].apply(year_category)\n",
    "        # # print(\"Construction year categorized:\\n\", self.dataset[['construction_category']].head())\n",
    "        \n",
    "        # # Apply one-hot encoding to construction_category\n",
    "        # self.dataset = pd.get_dummies(self.dataset, columns=['construction_category'], drop_first=True)\n",
    " \n",
    "        # # Convert all one-hot encoded columns to integers (0 and 1) \n",
    "        # one_hot_columns = [col for col in self.dataset.columns if 'construction_category_' in col]\n",
    "        # self.dataset[one_hot_columns] = self.dataset[one_hot_columns].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # def calculate_ratio(self):\n",
    "\n",
    "    #     #--------------------- \n",
    "    #     # calculate_terrace_ratio_apartment\n",
    "    #     if 'property_type' in self.dataset.columns:\n",
    "    #         # Prevent division by zero\n",
    "    #         self.dataset.loc[:, 'total_area_sqm'] = self.dataset['total_area_sqm'].replace(0, 1)\n",
    "\n",
    "    #         # Calculate terrace ratio for apartments\n",
    "    #         self.dataset.loc[:, 'terrace_ratio'] = np.where(\n",
    "    #             self.dataset['property_type'] == 'apartment',\n",
    "    #             self.dataset['terrace_sqm'] / self.dataset['total_area_sqm'],\n",
    "    #             np.nan\n",
    "    #         )\n",
    "    #         # Replace inf and -inf with NaN and fill NaNs with 0\n",
    "    #         self.dataset.loc[:, 'terrace_ratio'] = self.dataset['terrace_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    #         self.dataset.loc[:, 'terrace_ratio'] = self.dataset['terrace_ratio'].fillna(0)\n",
    "    #     else:\n",
    "    #         print(\"Warning: 'property_type' column not found.\")\n",
    "\n",
    "    #     #--------------------- \n",
    "    #     # calculate_land_area_ratio_house\n",
    "    #     if 'property_type' in self.dataset.columns:\n",
    "    #         # Prevent division by zero\n",
    "    #         self.dataset.loc[:, 'total_area_sqm'] = self.dataset['total_area_sqm'].replace(0, 1)\n",
    "\n",
    "    #         # Calculate land area ratio for houses\n",
    "    #         self.dataset.loc[:, 'land_area_ratio'] = np.where(\n",
    "    #             self.dataset['property_type'] == 'house',\n",
    "    #             self.dataset['surface_land_sqm'] / self.dataset['total_area_sqm'],\n",
    "    #             np.nan\n",
    "    #         )\n",
    "    #         # Replace inf and -inf with NaN and fill NaNs with 0\n",
    "    #         self.dataset.loc[:, 'land_area_ratio'] = self.dataset['land_area_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    #         self.dataset.loc[:, 'land_area_ratio'] = self.dataset['land_area_ratio'].fillna(0)\n",
    "    #     else:\n",
    "    #         print(\"Warning: 'property_type' column not found.\")\n",
    "\n",
    "     # ---------------------------------------------------------------------\n",
    "\n",
    "    def encode_zip_code(self):\n",
    "        \"\"\"\n",
    "        Encodes the 'zip_code' column using OneHotEncoder, adds the encoded columns to the dataset,\n",
    "        and creates a new 'zip_code_cut' column with the integer division of zip_code by 100.\n",
    "        \"\"\"\n",
    "        if 'zip_code' not in self.dataset.columns:\n",
    "            raise ValueError(\"The dataset does not have a 'zip_code' column.\")\n",
    "        \n",
    "        # Create a new column 'zip_code_cut' with zip_code // 100\n",
    "        self.dataset['zip_code_cut'] = self.dataset['zip_code'] // 100\n",
    "        \n",
    "        # Initialize the OneHotEncoder with the updated parameter\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        # Fit and transform the zip_code column\n",
    "        zip_code_encoded = encoder.fit_transform(self.dataset[['zip_code_cut']])\n",
    "        \n",
    "        # Create a DataFrame from the encoded zip codes with appropriate column names\n",
    "        encoded_df = pd.DataFrame(zip_code_encoded, columns=encoder.get_feature_names_out(['zip_code_cut']))\n",
    "        \n",
    "        # Combine the original dataset with the new encoded columns\n",
    "        self.dataset = pd.concat([self.dataset.reset_index(drop=True), encoded_df], axis=1)\n",
    "        \n",
    "        # Optionally drop the original 'zip_code' column if not needed\n",
    "        # self.dataset.drop(['zip_code'], axis=1, inplace=True)\n",
    "        \n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def process_advanced(self):\n",
    "\n",
    "        self.create_new_features()                         \n",
    "        self.categorize_construction_year() \n",
    "        self.encode_standardize_categories_features()\n",
    "        #self.calculate_ratio()\n",
    "        self.encode_zip_code()\n",
    "\n",
    "        return self.dataset\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "advanced_preprocessor = AdvancedPreprocessing(basic_preprocessed_data)\n",
    "advance_preprocessed_data = advanced_preprocessor.process_advanced()\n",
    "advance_preprocessed_data.to_csv(advance_preprocessed_dataset, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'property_type', 'zip_code', 'latitude', 'longitude',\n",
       "       'construction_year', 'total_area_sqm', 'surface_land_sqm',\n",
       "       'nbr_frontages', 'nbr_bedrooms',\n",
       "       ...\n",
       "       'zip_code_cut_90', 'zip_code_cut_91', 'zip_code_cut_92',\n",
       "       'zip_code_cut_93', 'zip_code_cut_94', 'zip_code_cut_95',\n",
       "       'zip_code_cut_96', 'zip_code_cut_97', 'zip_code_cut_98',\n",
       "       'zip_code_cut_99'],\n",
       "      dtype='object', length=126)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advance_preprocessed_data.columns\n",
    "#advance_preprocessed_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows with price less than or equal to 500,000 euros\n",
    "dataset_for_model = advance_preprocessed_data[advance_preprocessed_data['price'] <= 500_000]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
